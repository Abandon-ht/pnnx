7767517
68 67
Input                    pnnx_input_1             0 1 0 #0=(1,3,224,224)f32
nn.Conv2d                features.0               1 1 0 1 bias=True dilation=(1,1) groups=1 in_channels=3 kernel_size=(3,3) out_channels=64 padding=(0,0) stride=(2,2) @bias=(64)f32 @weight=(64,3,3,3)f32 #0=(1,3,224,224)f32 #1=(1,64,111,111)f32
nn.ReLU                  features.1               1 1 1 2 #1=(1,64,111,111)f32 #2=(1,64,111,111)f32
nn.MaxPool2d             features.2               1 1 2 3 ceil_mode=True dilation=(1,1) kernel_size=(3,3) padding=(0,0) return_indices=False stride=(2,2) #2=(1,64,111,111)f32 #3=(1,64,55,55)f32
nn.Conv2d                features.3.squeeze       1 1 3 4 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(1,1) out_channels=16 padding=(0,0) stride=(1,1) @bias=(16)f32 @weight=(16,64,1,1)f32 #3=(1,64,55,55)f32 #4=(1,16,55,55)f32
nn.ReLU                  features.3.squeeze_activation 1 1 4 5 #4=(1,16,55,55)f32 #5=(1,16,55,55)f32
nn.Conv2d                features.3.expand1x1     1 1 5 6 bias=True dilation=(1,1) groups=1 in_channels=16 kernel_size=(1,1) out_channels=64 padding=(0,0) stride=(1,1) @bias=(64)f32 @weight=(64,16,1,1)f32 #5=(1,16,55,55)f32 #6=(1,64,55,55)f32
nn.ReLU                  features.3.expand1x1_activation 1 1 6 7 #6=(1,64,55,55)f32 #7=(1,64,55,55)f32
nn.Conv2d                features.3.expand3x3     1 1 5 8 bias=True dilation=(1,1) groups=1 in_channels=16 kernel_size=(3,3) out_channels=64 padding=(1,1) stride=(1,1) @bias=(64)f32 @weight=(64,16,3,3)f32 #5=(1,16,55,55)f32 #8=(1,64,55,55)f32
nn.ReLU                  features.3.expand3x3_activation 1 1 8 9 #8=(1,64,55,55)f32 #9=(1,64,55,55)f32
torch.cat                torch.cat_7              2 1 7 9 10 dim=1 #7=(1,64,55,55)f32 #9=(1,64,55,55)f32 #10=(1,128,55,55)f32
nn.Conv2d                features.4.squeeze       1 1 10 11 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=16 padding=(0,0) stride=(1,1) @bias=(16)f32 @weight=(16,128,1,1)f32 #10=(1,128,55,55)f32 #11=(1,16,55,55)f32
nn.ReLU                  features.4.squeeze_activation 1 1 11 12 #11=(1,16,55,55)f32 #12=(1,16,55,55)f32
nn.Conv2d                features.4.expand1x1     1 1 12 13 bias=True dilation=(1,1) groups=1 in_channels=16 kernel_size=(1,1) out_channels=64 padding=(0,0) stride=(1,1) @bias=(64)f32 @weight=(64,16,1,1)f32 #12=(1,16,55,55)f32 #13=(1,64,55,55)f32
nn.ReLU                  features.4.expand1x1_activation 1 1 13 14 #13=(1,64,55,55)f32 #14=(1,64,55,55)f32
nn.Conv2d                features.4.expand3x3     1 1 12 15 bias=True dilation=(1,1) groups=1 in_channels=16 kernel_size=(3,3) out_channels=64 padding=(1,1) stride=(1,1) @bias=(64)f32 @weight=(64,16,3,3)f32 #12=(1,16,55,55)f32 #15=(1,64,55,55)f32
nn.ReLU                  features.4.expand3x3_activation 1 1 15 16 #15=(1,64,55,55)f32 #16=(1,64,55,55)f32
torch.cat                torch.cat_6              2 1 14 16 17 dim=1 #14=(1,64,55,55)f32 #16=(1,64,55,55)f32 #17=(1,128,55,55)f32
nn.MaxPool2d             features.5               1 1 17 18 ceil_mode=True dilation=(1,1) kernel_size=(3,3) padding=(0,0) return_indices=False stride=(2,2) #17=(1,128,55,55)f32 #18=(1,128,27,27)f32
nn.Conv2d                features.6.squeeze       1 1 18 19 bias=True dilation=(1,1) groups=1 in_channels=128 kernel_size=(1,1) out_channels=32 padding=(0,0) stride=(1,1) @bias=(32)f32 @weight=(32,128,1,1)f32 #18=(1,128,27,27)f32 #19=(1,32,27,27)f32
nn.ReLU                  features.6.squeeze_activation 1 1 19 20 #19=(1,32,27,27)f32 #20=(1,32,27,27)f32
nn.Conv2d                features.6.expand1x1     1 1 20 21 bias=True dilation=(1,1) groups=1 in_channels=32 kernel_size=(1,1) out_channels=128 padding=(0,0) stride=(1,1) @bias=(128)f32 @weight=(128,32,1,1)f32 #20=(1,32,27,27)f32 #21=(1,128,27,27)f32
nn.ReLU                  features.6.expand1x1_activation 1 1 21 22 #21=(1,128,27,27)f32 #22=(1,128,27,27)f32
nn.Conv2d                features.6.expand3x3     1 1 20 23 bias=True dilation=(1,1) groups=1 in_channels=32 kernel_size=(3,3) out_channels=128 padding=(1,1) stride=(1,1) @bias=(128)f32 @weight=(128,32,3,3)f32 #20=(1,32,27,27)f32 #23=(1,128,27,27)f32
nn.ReLU                  features.6.expand3x3_activation 1 1 23 24 #23=(1,128,27,27)f32 #24=(1,128,27,27)f32
torch.cat                torch.cat_5              2 1 22 24 25 dim=1 #22=(1,128,27,27)f32 #24=(1,128,27,27)f32 #25=(1,256,27,27)f32
nn.Conv2d                features.7.squeeze       1 1 25 26 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=32 padding=(0,0) stride=(1,1) @bias=(32)f32 @weight=(32,256,1,1)f32 #25=(1,256,27,27)f32 #26=(1,32,27,27)f32
nn.ReLU                  features.7.squeeze_activation 1 1 26 27 #26=(1,32,27,27)f32 #27=(1,32,27,27)f32
nn.Conv2d                features.7.expand1x1     1 1 27 28 bias=True dilation=(1,1) groups=1 in_channels=32 kernel_size=(1,1) out_channels=128 padding=(0,0) stride=(1,1) @bias=(128)f32 @weight=(128,32,1,1)f32 #27=(1,32,27,27)f32 #28=(1,128,27,27)f32
nn.ReLU                  features.7.expand1x1_activation 1 1 28 29 #28=(1,128,27,27)f32 #29=(1,128,27,27)f32
nn.Conv2d                features.7.expand3x3     1 1 27 30 bias=True dilation=(1,1) groups=1 in_channels=32 kernel_size=(3,3) out_channels=128 padding=(1,1) stride=(1,1) @bias=(128)f32 @weight=(128,32,3,3)f32 #27=(1,32,27,27)f32 #30=(1,128,27,27)f32
nn.ReLU                  features.7.expand3x3_activation 1 1 30 31 #30=(1,128,27,27)f32 #31=(1,128,27,27)f32
torch.cat                torch.cat_4              2 1 29 31 32 dim=1 #29=(1,128,27,27)f32 #31=(1,128,27,27)f32 #32=(1,256,27,27)f32
nn.MaxPool2d             features.8               1 1 32 33 ceil_mode=True dilation=(1,1) kernel_size=(3,3) padding=(0,0) return_indices=False stride=(2,2) #32=(1,256,27,27)f32 #33=(1,256,13,13)f32
nn.Conv2d                features.9.squeeze       1 1 33 34 bias=True dilation=(1,1) groups=1 in_channels=256 kernel_size=(1,1) out_channels=48 padding=(0,0) stride=(1,1) @bias=(48)f32 @weight=(48,256,1,1)f32 #33=(1,256,13,13)f32 #34=(1,48,13,13)f32
nn.ReLU                  features.9.squeeze_activation 1 1 34 35 #34=(1,48,13,13)f32 #35=(1,48,13,13)f32
nn.Conv2d                features.9.expand1x1     1 1 35 36 bias=True dilation=(1,1) groups=1 in_channels=48 kernel_size=(1,1) out_channels=192 padding=(0,0) stride=(1,1) @bias=(192)f32 @weight=(192,48,1,1)f32 #35=(1,48,13,13)f32 #36=(1,192,13,13)f32
nn.ReLU                  features.9.expand1x1_activation 1 1 36 37 #36=(1,192,13,13)f32 #37=(1,192,13,13)f32
nn.Conv2d                features.9.expand3x3     1 1 35 38 bias=True dilation=(1,1) groups=1 in_channels=48 kernel_size=(3,3) out_channels=192 padding=(1,1) stride=(1,1) @bias=(192)f32 @weight=(192,48,3,3)f32 #35=(1,48,13,13)f32 #38=(1,192,13,13)f32
nn.ReLU                  features.9.expand3x3_activation 1 1 38 39 #38=(1,192,13,13)f32 #39=(1,192,13,13)f32
torch.cat                torch.cat_3              2 1 37 39 40 dim=1 #37=(1,192,13,13)f32 #39=(1,192,13,13)f32 #40=(1,384,13,13)f32
nn.Conv2d                features.10.squeeze      1 1 40 41 bias=True dilation=(1,1) groups=1 in_channels=384 kernel_size=(1,1) out_channels=48 padding=(0,0) stride=(1,1) @bias=(48)f32 @weight=(48,384,1,1)f32 #40=(1,384,13,13)f32 #41=(1,48,13,13)f32
nn.ReLU                  features.10.squeeze_activation 1 1 41 42 #41=(1,48,13,13)f32 #42=(1,48,13,13)f32
nn.Conv2d                features.10.expand1x1    1 1 42 43 bias=True dilation=(1,1) groups=1 in_channels=48 kernel_size=(1,1) out_channels=192 padding=(0,0) stride=(1,1) @bias=(192)f32 @weight=(192,48,1,1)f32 #42=(1,48,13,13)f32 #43=(1,192,13,13)f32
nn.ReLU                  features.10.expand1x1_activation 1 1 43 44 #43=(1,192,13,13)f32 #44=(1,192,13,13)f32
nn.Conv2d                features.10.expand3x3    1 1 42 45 bias=True dilation=(1,1) groups=1 in_channels=48 kernel_size=(3,3) out_channels=192 padding=(1,1) stride=(1,1) @bias=(192)f32 @weight=(192,48,3,3)f32 #42=(1,48,13,13)f32 #45=(1,192,13,13)f32
nn.ReLU                  features.10.expand3x3_activation 1 1 45 46 #45=(1,192,13,13)f32 #46=(1,192,13,13)f32
torch.cat                torch.cat_2              2 1 44 46 47 dim=1 #44=(1,192,13,13)f32 #46=(1,192,13,13)f32 #47=(1,384,13,13)f32
nn.Conv2d                features.11.squeeze      1 1 47 48 bias=True dilation=(1,1) groups=1 in_channels=384 kernel_size=(1,1) out_channels=64 padding=(0,0) stride=(1,1) @bias=(64)f32 @weight=(64,384,1,1)f32 #47=(1,384,13,13)f32 #48=(1,64,13,13)f32
nn.ReLU                  features.11.squeeze_activation 1 1 48 49 #48=(1,64,13,13)f32 #49=(1,64,13,13)f32
nn.Conv2d                features.11.expand1x1    1 1 49 50 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(1,1) out_channels=256 padding=(0,0) stride=(1,1) @bias=(256)f32 @weight=(256,64,1,1)f32 #49=(1,64,13,13)f32 #50=(1,256,13,13)f32
nn.ReLU                  features.11.expand1x1_activation 1 1 50 51 #50=(1,256,13,13)f32 #51=(1,256,13,13)f32
nn.Conv2d                features.11.expand3x3    1 1 49 52 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=256 padding=(1,1) stride=(1,1) @bias=(256)f32 @weight=(256,64,3,3)f32 #49=(1,64,13,13)f32 #52=(1,256,13,13)f32
nn.ReLU                  features.11.expand3x3_activation 1 1 52 53 #52=(1,256,13,13)f32 #53=(1,256,13,13)f32
torch.cat                torch.cat_1              2 1 51 53 54 dim=1 #51=(1,256,13,13)f32 #53=(1,256,13,13)f32 #54=(1,512,13,13)f32
nn.Conv2d                features.12.squeeze      1 1 54 55 bias=True dilation=(1,1) groups=1 in_channels=512 kernel_size=(1,1) out_channels=64 padding=(0,0) stride=(1,1) @bias=(64)f32 @weight=(64,512,1,1)f32 #54=(1,512,13,13)f32 #55=(1,64,13,13)f32
nn.ReLU                  features.12.squeeze_activation 1 1 55 56 #55=(1,64,13,13)f32 #56=(1,64,13,13)f32
nn.Conv2d                features.12.expand1x1    1 1 56 57 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(1,1) out_channels=256 padding=(0,0) stride=(1,1) @bias=(256)f32 @weight=(256,64,1,1)f32 #56=(1,64,13,13)f32 #57=(1,256,13,13)f32
nn.ReLU                  features.12.expand1x1_activation 1 1 57 58 #57=(1,256,13,13)f32 #58=(1,256,13,13)f32
nn.Conv2d                features.12.expand3x3    1 1 56 59 bias=True dilation=(1,1) groups=1 in_channels=64 kernel_size=(3,3) out_channels=256 padding=(1,1) stride=(1,1) @bias=(256)f32 @weight=(256,64,3,3)f32 #56=(1,64,13,13)f32 #59=(1,256,13,13)f32
nn.ReLU                  features.12.expand3x3_activation 1 1 59 60 #59=(1,256,13,13)f32 #60=(1,256,13,13)f32
torch.cat                torch.cat_0              2 1 58 60 61 dim=1 #58=(1,256,13,13)f32 #60=(1,256,13,13)f32 #61=(1,512,13,13)f32
nn.Dropout               classifier.0             1 1 61 62 #61=(1,512,13,13)f32 #62=(1,512,13,13)f32
nn.Conv2d                classifier.1             1 1 62 63 bias=True dilation=(1,1) groups=1 in_channels=512 kernel_size=(1,1) out_channels=1000 padding=(0,0) stride=(1,1) @bias=(1000)f32 @weight=(1000,512,1,1)f32 #62=(1,512,13,13)f32 #63=(1,1000,13,13)f32
nn.ReLU                  classifier.2             1 1 63 64 #63=(1,1000,13,13)f32 #64=(1,1000,13,13)f32
nn.AdaptiveAvgPool2d     classifier.3             1 1 64 65 output_size=(1,1) #64=(1,1000,13,13)f32 #65=(1,1000,1,1)f32
torch.flatten            torch.flatten_8          1 1 65 66 end_dim=-1 start_dim=1 $input=65 #65=(1,1000,1,1)f32 #66=(1,1000)f32
Output                   pnnx_output_0            1 0 66 #66=(1,1000)f32
