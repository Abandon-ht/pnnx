7767517
143 155
Input                    pnnx_input_1             0 1 0
nn.Conv2d                conv2d_batchnorm2d_55    1 1 0 1 bias=True dilation=(1,1) groups=1 in_channels=3 kernel_size=(3,3) out_channels=24 padding=(1,1) stride=(2,2) @bias=(24)f32 @weight=(24,3,3,3)f32 $input=0
nn.ReLU                  conv1.2                  1 1 1 2
nn.MaxPool2d             maxpool                  1 1 2 3 ceil_mode=False dilation=(1,1) kernel_size=(3,3) padding=(1,1) return_indices=False stride=(2,2)
nn.Conv2d                conv2d_batchnorm2d_54    1 1 3 4 bias=True dilation=(1,1) groups=24 in_channels=24 kernel_size=(3,3) out_channels=24 padding=(1,1) stride=(2,2) @bias=(24)f32 @weight=(24,1,3,3)f32 $input=3
nn.Conv2d                conv2d_batchnorm2d_53    1 1 4 5 bias=True dilation=(1,1) groups=1 in_channels=24 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @bias=(58)f32 @weight=(58,24,1,1)f32 $input=4
nn.ReLU                  stage2.0.branch1.4       1 1 5 6
nn.Conv2d                conv2d_batchnorm2d_52    1 1 3 7 bias=True dilation=(1,1) groups=1 in_channels=24 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @bias=(58)f32 @weight=(58,24,1,1)f32 $input=3
nn.ReLU                  stage2.0.branch2.2       1 1 7 8
nn.Conv2d                conv2d_batchnorm2d_51    1 1 8 9 bias=True dilation=(1,1) groups=58 in_channels=58 kernel_size=(3,3) out_channels=58 padding=(1,1) stride=(2,2) @bias=(58)f32 @weight=(58,1,3,3)f32 $input=8
nn.Conv2d                conv2d_batchnorm2d_50    1 1 9 10 bias=True dilation=(1,1) groups=1 in_channels=58 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @bias=(58)f32 @weight=(58,58,1,1)f32 $input=9
nn.ReLU                  stage2.0.branch2.7       1 1 10 11
torch.cat                torch.cat_63             2 1 6 11 12 dim=1
torch.transpose          torch.transpose_93       1 1 12 13 dim0=1 dim1=2 $input=12
torch.chunk              torch.chunk_76           1 2 13 14 15 chunks=2 dim=1 $input=13
nn.Conv2d                conv2d_batchnorm2d_49    1 1 15 16 bias=True dilation=(1,1) groups=1 in_channels=58 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @bias=(58)f32 @weight=(58,58,1,1)f32 $input=15
nn.ReLU                  stage2.1.branch2.2       1 1 16 17
nn.Conv2d                conv2d_batchnorm2d_48    1 1 17 18 bias=True dilation=(1,1) groups=58 in_channels=58 kernel_size=(3,3) out_channels=58 padding=(1,1) stride=(1,1) @bias=(58)f32 @weight=(58,1,3,3)f32 $input=17
nn.Conv2d                conv2d_batchnorm2d_47    1 1 18 19 bias=True dilation=(1,1) groups=1 in_channels=58 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @bias=(58)f32 @weight=(58,58,1,1)f32 $input=18
nn.ReLU                  stage2.1.branch2.7       1 1 19 20
torch.cat                torch.cat_62             2 1 14 20 21 dim=1
torch.transpose          torch.transpose_92       1 1 21 22 dim0=1 dim1=2 $input=21
torch.chunk              torch.chunk_75           1 2 22 23 24 chunks=2 dim=1 $input=22
nn.Conv2d                conv2d_batchnorm2d_46    1 1 24 25 bias=True dilation=(1,1) groups=1 in_channels=58 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @bias=(58)f32 @weight=(58,58,1,1)f32 $input=24
nn.ReLU                  stage2.2.branch2.2       1 1 25 26
nn.Conv2d                conv2d_batchnorm2d_45    1 1 26 27 bias=True dilation=(1,1) groups=58 in_channels=58 kernel_size=(3,3) out_channels=58 padding=(1,1) stride=(1,1) @bias=(58)f32 @weight=(58,1,3,3)f32 $input=26
nn.Conv2d                conv2d_batchnorm2d_44    1 1 27 28 bias=True dilation=(1,1) groups=1 in_channels=58 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @bias=(58)f32 @weight=(58,58,1,1)f32 $input=27
nn.ReLU                  stage2.2.branch2.7       1 1 28 29
torch.cat                torch.cat_61             2 1 23 29 30 dim=1
torch.transpose          torch.transpose_91       1 1 30 31 dim0=1 dim1=2 $input=30
torch.chunk              torch.chunk_74           1 2 31 32 33 chunks=2 dim=1 $input=31
nn.Conv2d                conv2d_batchnorm2d_43    1 1 33 34 bias=True dilation=(1,1) groups=1 in_channels=58 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @bias=(58)f32 @weight=(58,58,1,1)f32 $input=33
nn.ReLU                  stage2.3.branch2.2       1 1 34 35
nn.Conv2d                conv2d_batchnorm2d_42    1 1 35 36 bias=True dilation=(1,1) groups=58 in_channels=58 kernel_size=(3,3) out_channels=58 padding=(1,1) stride=(1,1) @bias=(58)f32 @weight=(58,1,3,3)f32 $input=35
nn.Conv2d                conv2d_batchnorm2d_41    1 1 36 37 bias=True dilation=(1,1) groups=1 in_channels=58 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @bias=(58)f32 @weight=(58,58,1,1)f32 $input=36
nn.ReLU                  stage2.3.branch2.7       1 1 37 38
torch.cat                torch.cat_60             2 1 32 38 39 dim=1
torch.transpose          torch.transpose_90       1 1 39 40 dim0=1 dim1=2 $input=39
nn.Conv2d                conv2d_batchnorm2d_40    1 1 40 41 bias=True dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(2,2) @bias=(116)f32 @weight=(116,1,3,3)f32 $input=40
nn.Conv2d                conv2d_batchnorm2d_39    1 1 41 42 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=41
nn.ReLU                  stage3.0.branch1.4       1 1 42 43
nn.Conv2d                conv2d_batchnorm2d_38    1 1 40 44 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=40
nn.ReLU                  stage3.0.branch2.2       1 1 44 45
nn.Conv2d                conv2d_batchnorm2d_37    1 1 45 46 bias=True dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(2,2) @bias=(116)f32 @weight=(116,1,3,3)f32 $input=45
nn.Conv2d                conv2d_batchnorm2d_36    1 1 46 47 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=46
nn.ReLU                  stage3.0.branch2.7       1 1 47 48
torch.cat                torch.cat_59             2 1 43 48 49 dim=1
torch.transpose          torch.transpose_89       1 1 49 50 dim0=1 dim1=2 $input=49
torch.chunk              torch.chunk_73           1 2 50 51 52 chunks=2 dim=1 $input=50
nn.Conv2d                conv2d_batchnorm2d_35    1 1 52 53 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=52
nn.ReLU                  stage3.1.branch2.2       1 1 53 54
nn.Conv2d                conv2d_batchnorm2d_34    1 1 54 55 bias=True dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(1,1) @bias=(116)f32 @weight=(116,1,3,3)f32 $input=54
nn.Conv2d                conv2d_batchnorm2d_33    1 1 55 56 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=55
nn.ReLU                  stage3.1.branch2.7       1 1 56 57
torch.cat                torch.cat_58             2 1 51 57 58 dim=1
torch.transpose          torch.transpose_88       1 1 58 59 dim0=1 dim1=2 $input=58
torch.chunk              torch.chunk_72           1 2 59 60 61 chunks=2 dim=1 $input=59
nn.Conv2d                conv2d_batchnorm2d_32    1 1 61 62 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=61
nn.ReLU                  stage3.2.branch2.2       1 1 62 63
nn.Conv2d                conv2d_batchnorm2d_31    1 1 63 64 bias=True dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(1,1) @bias=(116)f32 @weight=(116,1,3,3)f32 $input=63
nn.Conv2d                conv2d_batchnorm2d_30    1 1 64 65 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=64
nn.ReLU                  stage3.2.branch2.7       1 1 65 66
torch.cat                torch.cat_57             2 1 60 66 67 dim=1
torch.transpose          torch.transpose_87       1 1 67 68 dim0=1 dim1=2 $input=67
torch.chunk              torch.chunk_71           1 2 68 69 70 chunks=2 dim=1 $input=68
nn.Conv2d                conv2d_batchnorm2d_29    1 1 70 71 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=70
nn.ReLU                  stage3.3.branch2.2       1 1 71 72
nn.Conv2d                conv2d_batchnorm2d_28    1 1 72 73 bias=True dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(1,1) @bias=(116)f32 @weight=(116,1,3,3)f32 $input=72
nn.Conv2d                conv2d_batchnorm2d_27    1 1 73 74 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=73
nn.ReLU                  stage3.3.branch2.7       1 1 74 75
torch.cat                torch.cat_56             2 1 69 75 76 dim=1
torch.transpose          torch.transpose_86       1 1 76 77 dim0=1 dim1=2 $input=76
torch.chunk              torch.chunk_70           1 2 77 78 79 chunks=2 dim=1 $input=77
nn.Conv2d                conv2d_batchnorm2d_26    1 1 79 80 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=79
nn.ReLU                  stage3.4.branch2.2       1 1 80 81
nn.Conv2d                conv2d_batchnorm2d_25    1 1 81 82 bias=True dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(1,1) @bias=(116)f32 @weight=(116,1,3,3)f32 $input=81
nn.Conv2d                conv2d_batchnorm2d_24    1 1 82 83 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=82
nn.ReLU                  stage3.4.branch2.7       1 1 83 84
torch.cat                torch.cat_55             2 1 78 84 85 dim=1
torch.transpose          torch.transpose_85       1 1 85 86 dim0=1 dim1=2 $input=85
torch.chunk              torch.chunk_69           1 2 86 87 88 chunks=2 dim=1 $input=86
nn.Conv2d                conv2d_batchnorm2d_23    1 1 88 89 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=88
nn.ReLU                  stage3.5.branch2.2       1 1 89 90
nn.Conv2d                conv2d_batchnorm2d_22    1 1 90 91 bias=True dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(1,1) @bias=(116)f32 @weight=(116,1,3,3)f32 $input=90
nn.Conv2d                conv2d_batchnorm2d_21    1 1 91 92 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=91
nn.ReLU                  stage3.5.branch2.7       1 1 92 93
torch.cat                torch.cat_54             2 1 87 93 94 dim=1
torch.transpose          torch.transpose_84       1 1 94 95 dim0=1 dim1=2 $input=94
torch.chunk              torch.chunk_68           1 2 95 96 97 chunks=2 dim=1 $input=95
nn.Conv2d                conv2d_batchnorm2d_20    1 1 97 98 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=97
nn.ReLU                  stage3.6.branch2.2       1 1 98 99
nn.Conv2d                conv2d_batchnorm2d_19    1 1 99 100 bias=True dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(1,1) @bias=(116)f32 @weight=(116,1,3,3)f32 $input=99
nn.Conv2d                conv2d_batchnorm2d_18    1 1 100 101 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=100
nn.ReLU                  stage3.6.branch2.7       1 1 101 102
torch.cat                torch.cat_53             2 1 96 102 103 dim=1
torch.transpose          torch.transpose_83       1 1 103 104 dim0=1 dim1=2 $input=103
torch.chunk              torch.chunk_67           1 2 104 105 106 chunks=2 dim=1 $input=104
nn.Conv2d                conv2d_batchnorm2d_17    1 1 106 107 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=106
nn.ReLU                  stage3.7.branch2.2       1 1 107 108
nn.Conv2d                conv2d_batchnorm2d_16    1 1 108 109 bias=True dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(1,1) @bias=(116)f32 @weight=(116,1,3,3)f32 $input=108
nn.Conv2d                conv2d_batchnorm2d_15    1 1 109 110 bias=True dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @bias=(116)f32 @weight=(116,116,1,1)f32 $input=109
nn.ReLU                  stage3.7.branch2.7       1 1 110 111
torch.cat                torch.cat_52             2 1 105 111 112 dim=1
torch.transpose          torch.transpose_82       1 1 112 113 dim0=1 dim1=2 $input=112
nn.Conv2d                conv2d_batchnorm2d_14    1 1 113 114 bias=True dilation=(1,1) groups=232 in_channels=232 kernel_size=(3,3) out_channels=232 padding=(1,1) stride=(2,2) @bias=(232)f32 @weight=(232,1,3,3)f32 $input=113
nn.Conv2d                conv2d_batchnorm2d_13    1 1 114 115 bias=True dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @bias=(232)f32 @weight=(232,232,1,1)f32 $input=114
nn.ReLU                  stage4.0.branch1.4       1 1 115 116
nn.Conv2d                conv2d_batchnorm2d_12    1 1 113 117 bias=True dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @bias=(232)f32 @weight=(232,232,1,1)f32 $input=113
nn.ReLU                  stage4.0.branch2.2       1 1 117 118
nn.Conv2d                conv2d_batchnorm2d_11    1 1 118 119 bias=True dilation=(1,1) groups=232 in_channels=232 kernel_size=(3,3) out_channels=232 padding=(1,1) stride=(2,2) @bias=(232)f32 @weight=(232,1,3,3)f32 $input=118
nn.Conv2d                conv2d_batchnorm2d_10    1 1 119 120 bias=True dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @bias=(232)f32 @weight=(232,232,1,1)f32 $input=119
nn.ReLU                  stage4.0.branch2.7       1 1 120 121
torch.cat                torch.cat_51             2 1 116 121 122 dim=1
torch.transpose          torch.transpose_81       1 1 122 123 dim0=1 dim1=2 $input=122
torch.chunk              torch.chunk_66           1 2 123 124 125 chunks=2 dim=1 $input=123
nn.Conv2d                conv2d_batchnorm2d_9     1 1 125 126 bias=True dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @bias=(232)f32 @weight=(232,232,1,1)f32 $input=125
nn.ReLU                  stage4.1.branch2.2       1 1 126 127
nn.Conv2d                conv2d_batchnorm2d_8     1 1 127 128 bias=True dilation=(1,1) groups=232 in_channels=232 kernel_size=(3,3) out_channels=232 padding=(1,1) stride=(1,1) @bias=(232)f32 @weight=(232,1,3,3)f32 $input=127
nn.Conv2d                conv2d_batchnorm2d_7     1 1 128 129 bias=True dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @bias=(232)f32 @weight=(232,232,1,1)f32 $input=128
nn.ReLU                  stage4.1.branch2.7       1 1 129 130
torch.cat                torch.cat_50             2 1 124 130 131 dim=1
torch.transpose          torch.transpose_80       1 1 131 132 dim0=1 dim1=2 $input=131
torch.chunk              torch.chunk_65           1 2 132 133 134 chunks=2 dim=1 $input=132
nn.Conv2d                conv2d_batchnorm2d_6     1 1 134 135 bias=True dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @bias=(232)f32 @weight=(232,232,1,1)f32 $input=134
nn.ReLU                  stage4.2.branch2.2       1 1 135 136
nn.Conv2d                conv2d_batchnorm2d_5     1 1 136 137 bias=True dilation=(1,1) groups=232 in_channels=232 kernel_size=(3,3) out_channels=232 padding=(1,1) stride=(1,1) @bias=(232)f32 @weight=(232,1,3,3)f32 $input=136
nn.Conv2d                conv2d_batchnorm2d_4     1 1 137 138 bias=True dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @bias=(232)f32 @weight=(232,232,1,1)f32 $input=137
nn.ReLU                  stage4.2.branch2.7       1 1 138 139
torch.cat                torch.cat_49             2 1 133 139 140 dim=1
torch.transpose          torch.transpose_79       1 1 140 141 dim0=1 dim1=2 $input=140
torch.chunk              torch.chunk_64           1 2 141 142 143 chunks=2 dim=1 $input=141
nn.Conv2d                conv2d_batchnorm2d_3     1 1 143 144 bias=True dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @bias=(232)f32 @weight=(232,232,1,1)f32 $input=143
nn.ReLU                  stage4.3.branch2.2       1 1 144 145
nn.Conv2d                conv2d_batchnorm2d_2     1 1 145 146 bias=True dilation=(1,1) groups=232 in_channels=232 kernel_size=(3,3) out_channels=232 padding=(1,1) stride=(1,1) @bias=(232)f32 @weight=(232,1,3,3)f32 $input=145
nn.Conv2d                conv2d_batchnorm2d_1     1 1 146 147 bias=True dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @bias=(232)f32 @weight=(232,232,1,1)f32 $input=146
nn.ReLU                  stage4.3.branch2.7       1 1 147 148
torch.cat                torch.cat_48             2 1 142 148 149 dim=1
torch.transpose          torch.transpose_78       1 1 149 150 dim0=1 dim1=2 $input=149
nn.Conv2d                conv2d_batchnorm2d_0     1 1 150 151 bias=True dilation=(1,1) groups=1 in_channels=464 kernel_size=(1,1) out_channels=1024 padding=(0,0) stride=(1,1) @bias=(1024)f32 @weight=(1024,464,1,1)f32 $input=150
nn.ReLU                  conv5.2                  1 1 151 152
torch.mean               torch.mean_77            1 1 152 153 dim=(2,3) keepdim=False $input=152
nn.Linear                fc                       1 1 153 154 bias=True in_features=1024 out_features=1000 @bias=(1000)f32 @weight=(1000,1024)f32
Output                   pnnx_output_0            1 0 154
