7767517
231 243
Input                    pnnx_input_1             0 1 x.3 #x.3=(1,3,224,224)f32
Conv2d                   conv1.0                  1 1 x.3 51 bias=0 dilation=(1,1) groups=1 in_channels=3 kernel_size=(3,3) out_channels=24 padding=(1,1) stride=(2,2) @weight=(24,3,3,3)f32 #x.3=(1,3,224,224)f32 #51=(1,24,112,112)f32
BatchNorm2d              conv1.1                  1 1 51 52 affine=1 eps=1.000000e-05 num_features=24 @bias=(24)f32 @running_mean=(24)f32 @running_var=(24)f32 @weight=(24)f32 #51=(1,24,112,112)f32 #52=(1,24,112,112)f32
ReLU                     conv1.2                  1 1 52 53 #52=(1,24,112,112)f32 #53=(1,24,112,112)f32
MaxPool2d                maxpool                  1 1 53 18 ceil_mode=0 dilation=(1,1) kernel_size=(3,3) padding=(1,1) stride=(2,2) #53=(1,24,112,112)f32 #18=(1,24,56,56)f32
Conv2d                   stage2.0.branch1.0       1 1 18 654 bias=0 dilation=(1,1) groups=24 in_channels=24 kernel_size=(3,3) out_channels=24 padding=(1,1) stride=(2,2) @weight=(24,1,3,3)f32 #18=(1,24,56,56)f32 #654=(1,24,28,28)f32
BatchNorm2d              stage2.0.branch1.1       1 1 654 655 affine=1 eps=1.000000e-05 num_features=24 @bias=(24)f32 @running_mean=(24)f32 @running_var=(24)f32 @weight=(24)f32 #654=(1,24,28,28)f32 #655=(1,24,28,28)f32
Conv2d                   stage2.0.branch1.2       1 1 655 656 bias=0 dilation=(1,1) groups=1 in_channels=24 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @weight=(58,24,1,1)f32 #655=(1,24,28,28)f32 #656=(1,58,28,28)f32
BatchNorm2d              stage2.0.branch1.3       1 1 656 657 affine=1 eps=1.000000e-05 num_features=58 @bias=(58)f32 @running_mean=(58)f32 @running_var=(58)f32 @weight=(58)f32 #656=(1,58,28,28)f32 #657=(1,58,28,28)f32
ReLU                     stage2.0.branch1.4       1 1 657 658 #657=(1,58,28,28)f32 #658=(1,58,28,28)f32
Conv2d                   stage2.0.branch2.0       1 1 18 667 bias=0 dilation=(1,1) groups=1 in_channels=24 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @weight=(58,24,1,1)f32 #18=(1,24,56,56)f32 #667=(1,58,56,56)f32
BatchNorm2d              stage2.0.branch2.1       1 1 667 668 affine=1 eps=1.000000e-05 num_features=58 @bias=(58)f32 @running_mean=(58)f32 @running_var=(58)f32 @weight=(58)f32 #667=(1,58,56,56)f32 #668=(1,58,56,56)f32
ReLU                     stage2.0.branch2.2       1 1 668 669 #668=(1,58,56,56)f32 #669=(1,58,56,56)f32
Conv2d                   stage2.0.branch2.3       1 1 669 670 bias=0 dilation=(1,1) groups=58 in_channels=58 kernel_size=(3,3) out_channels=58 padding=(1,1) stride=(2,2) @weight=(58,1,3,3)f32 #669=(1,58,56,56)f32 #670=(1,58,28,28)f32
BatchNorm2d              stage2.0.branch2.4       1 1 670 671 affine=1 eps=1.000000e-05 num_features=58 @bias=(58)f32 @running_mean=(58)f32 @running_var=(58)f32 @weight=(58)f32 #670=(1,58,28,28)f32 #671=(1,58,28,28)f32
Conv2d                   stage2.0.branch2.5       1 1 671 672 bias=0 dilation=(1,1) groups=1 in_channels=58 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @weight=(58,58,1,1)f32 #671=(1,58,28,28)f32 #672=(1,58,28,28)f32
BatchNorm2d              stage2.0.branch2.6       1 1 672 673 affine=1 eps=1.000000e-05 num_features=58 @bias=(58)f32 @running_mean=(58)f32 @running_var=(58)f32 @weight=(58)f32 #672=(1,58,28,28)f32 #673=(1,58,28,28)f32
ReLU                     stage2.0.branch2.7       1 1 673 674 #673=(1,58,28,28)f32 #674=(1,58,28,28)f32
torch.cat                pnnx_9                   2 1 658 674 x.5 dim=1 #658=(1,58,28,28)f32 #674=(1,58,28,28)f32 #x.5=(1,116,28,28)f32
aten::view               pnnx_29                  1 1 x.5 x0.2 shape=[1,2,58,28,28] #x.5=(1,116,28,28)f32 #x0.2=(1,2,58,28,28)f32
torch.transpose          pnnx_32                  1 1 x0.2 x1.2 dim0=1 dim1=2 #x0.2=(1,2,58,28,28)f32 #x1.2=(1,58,2,28,28)f32
aten::view               pnnx_34                  1 1 x1.2 125 shape=[1,-1,28,28] #x1.2=(1,58,2,28,28)f32 #125=(1,116,28,28)f32
torch.chunk              pnnx_38                  1 2 125 x1.4 input.5 chunks=2 dim=1 #125=(1,116,28,28)f32 #x1.4=(1,58,28,28)f32 #input.5=(1,58,28,28)f32
Conv2d                   stage2.1.branch2.0       1 1 input.5 683 bias=0 dilation=(1,1) groups=1 in_channels=58 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @weight=(58,58,1,1)f32 #input.5=(1,58,28,28)f32 #683=(1,58,28,28)f32
BatchNorm2d              stage2.1.branch2.1       1 1 683 684 affine=1 eps=1.000000e-05 num_features=58 @bias=(58)f32 @running_mean=(58)f32 @running_var=(58)f32 @weight=(58)f32 #683=(1,58,28,28)f32 #684=(1,58,28,28)f32
ReLU                     stage2.1.branch2.2       1 1 684 685 #684=(1,58,28,28)f32 #685=(1,58,28,28)f32
Conv2d                   stage2.1.branch2.3       1 1 685 686 bias=0 dilation=(1,1) groups=58 in_channels=58 kernel_size=(3,3) out_channels=58 padding=(1,1) stride=(1,1) @weight=(58,1,3,3)f32 #685=(1,58,28,28)f32 #686=(1,58,28,28)f32
BatchNorm2d              stage2.1.branch2.4       1 1 686 687 affine=1 eps=1.000000e-05 num_features=58 @bias=(58)f32 @running_mean=(58)f32 @running_var=(58)f32 @weight=(58)f32 #686=(1,58,28,28)f32 #687=(1,58,28,28)f32
Conv2d                   stage2.1.branch2.5       1 1 687 688 bias=0 dilation=(1,1) groups=1 in_channels=58 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @weight=(58,58,1,1)f32 #687=(1,58,28,28)f32 #688=(1,58,28,28)f32
BatchNorm2d              stage2.1.branch2.6       1 1 688 689 affine=1 eps=1.000000e-05 num_features=58 @bias=(58)f32 @running_mean=(58)f32 @running_var=(58)f32 @weight=(58)f32 #688=(1,58,28,28)f32 #689=(1,58,28,28)f32
ReLU                     stage2.1.branch2.7       1 1 689 690 #689=(1,58,28,28)f32 #690=(1,58,28,28)f32
torch.cat                pnnx_43                  2 1 x1.4 690 x.7 dim=1 #x1.4=(1,58,28,28)f32 #690=(1,58,28,28)f32 #x.7=(1,116,28,28)f32
aten::view               pnnx_64                  1 1 x.7 x0.4 shape=[1,2,58,28,28] #x.7=(1,116,28,28)f32 #x0.4=(1,2,58,28,28)f32
torch.transpose          pnnx_67                  1 1 x0.4 x2.2 dim0=1 dim1=2 #x0.4=(1,2,58,28,28)f32 #x2.2=(1,58,2,28,28)f32
aten::view               pnnx_69                  1 1 x2.2 160 shape=[1,-1,28,28] #x2.2=(1,58,2,28,28)f32 #160=(1,116,28,28)f32
torch.chunk              pnnx_73                  1 2 160 x1.6 input.7 chunks=2 dim=1 #160=(1,116,28,28)f32 #x1.6=(1,58,28,28)f32 #input.7=(1,58,28,28)f32
Conv2d                   stage2.2.branch2.0       1 1 input.7 699 bias=0 dilation=(1,1) groups=1 in_channels=58 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @weight=(58,58,1,1)f32 #input.7=(1,58,28,28)f32 #699=(1,58,28,28)f32
BatchNorm2d              stage2.2.branch2.1       1 1 699 700 affine=1 eps=1.000000e-05 num_features=58 @bias=(58)f32 @running_mean=(58)f32 @running_var=(58)f32 @weight=(58)f32 #699=(1,58,28,28)f32 #700=(1,58,28,28)f32
ReLU                     stage2.2.branch2.2       1 1 700 701 #700=(1,58,28,28)f32 #701=(1,58,28,28)f32
Conv2d                   stage2.2.branch2.3       1 1 701 702 bias=0 dilation=(1,1) groups=58 in_channels=58 kernel_size=(3,3) out_channels=58 padding=(1,1) stride=(1,1) @weight=(58,1,3,3)f32 #701=(1,58,28,28)f32 #702=(1,58,28,28)f32
BatchNorm2d              stage2.2.branch2.4       1 1 702 703 affine=1 eps=1.000000e-05 num_features=58 @bias=(58)f32 @running_mean=(58)f32 @running_var=(58)f32 @weight=(58)f32 #702=(1,58,28,28)f32 #703=(1,58,28,28)f32
Conv2d                   stage2.2.branch2.5       1 1 703 704 bias=0 dilation=(1,1) groups=1 in_channels=58 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @weight=(58,58,1,1)f32 #703=(1,58,28,28)f32 #704=(1,58,28,28)f32
BatchNorm2d              stage2.2.branch2.6       1 1 704 705 affine=1 eps=1.000000e-05 num_features=58 @bias=(58)f32 @running_mean=(58)f32 @running_var=(58)f32 @weight=(58)f32 #704=(1,58,28,28)f32 #705=(1,58,28,28)f32
ReLU                     stage2.2.branch2.7       1 1 705 706 #705=(1,58,28,28)f32 #706=(1,58,28,28)f32
torch.cat                pnnx_78                  2 1 x1.6 706 x.9 dim=1 #x1.6=(1,58,28,28)f32 #706=(1,58,28,28)f32 #x.9=(1,116,28,28)f32
aten::view               pnnx_99                  1 1 x.9 x0.6 shape=[1,2,58,28,28] #x.9=(1,116,28,28)f32 #x0.6=(1,2,58,28,28)f32
torch.transpose          pnnx_102                 1 1 x0.6 x2.4 dim0=1 dim1=2 #x0.6=(1,2,58,28,28)f32 #x2.4=(1,58,2,28,28)f32
aten::view               pnnx_104                 1 1 x2.4 195 shape=[1,-1,28,28] #x2.4=(1,58,2,28,28)f32 #195=(1,116,28,28)f32
torch.chunk              pnnx_108                 1 2 195 x1.8 input.9 chunks=2 dim=1 #195=(1,116,28,28)f32 #x1.8=(1,58,28,28)f32 #input.9=(1,58,28,28)f32
Conv2d                   stage2.3.branch2.0       1 1 input.9 715 bias=0 dilation=(1,1) groups=1 in_channels=58 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @weight=(58,58,1,1)f32 #input.9=(1,58,28,28)f32 #715=(1,58,28,28)f32
BatchNorm2d              stage2.3.branch2.1       1 1 715 716 affine=1 eps=1.000000e-05 num_features=58 @bias=(58)f32 @running_mean=(58)f32 @running_var=(58)f32 @weight=(58)f32 #715=(1,58,28,28)f32 #716=(1,58,28,28)f32
ReLU                     stage2.3.branch2.2       1 1 716 717 #716=(1,58,28,28)f32 #717=(1,58,28,28)f32
Conv2d                   stage2.3.branch2.3       1 1 717 718 bias=0 dilation=(1,1) groups=58 in_channels=58 kernel_size=(3,3) out_channels=58 padding=(1,1) stride=(1,1) @weight=(58,1,3,3)f32 #717=(1,58,28,28)f32 #718=(1,58,28,28)f32
BatchNorm2d              stage2.3.branch2.4       1 1 718 719 affine=1 eps=1.000000e-05 num_features=58 @bias=(58)f32 @running_mean=(58)f32 @running_var=(58)f32 @weight=(58)f32 #718=(1,58,28,28)f32 #719=(1,58,28,28)f32
Conv2d                   stage2.3.branch2.5       1 1 719 720 bias=0 dilation=(1,1) groups=1 in_channels=58 kernel_size=(1,1) out_channels=58 padding=(0,0) stride=(1,1) @weight=(58,58,1,1)f32 #719=(1,58,28,28)f32 #720=(1,58,28,28)f32
BatchNorm2d              stage2.3.branch2.6       1 1 720 721 affine=1 eps=1.000000e-05 num_features=58 @bias=(58)f32 @running_mean=(58)f32 @running_var=(58)f32 @weight=(58)f32 #720=(1,58,28,28)f32 #721=(1,58,28,28)f32
ReLU                     stage2.3.branch2.7       1 1 721 722 #721=(1,58,28,28)f32 #722=(1,58,28,28)f32
torch.cat                pnnx_113                 2 1 x1.8 722 x.11 dim=1 #x1.8=(1,58,28,28)f32 #722=(1,58,28,28)f32 #x.11=(1,116,28,28)f32
aten::view               pnnx_134                 1 1 x.11 x0.8 shape=[1,2,58,28,28] #x.11=(1,116,28,28)f32 #x0.8=(1,2,58,28,28)f32
torch.transpose          pnnx_137                 1 1 x0.8 x2.6 dim0=1 dim1=2 #x0.8=(1,2,58,28,28)f32 #x2.6=(1,58,2,28,28)f32
aten::view               pnnx_139                 1 1 x2.6 230 shape=[1,-1,28,28] #x2.6=(1,58,2,28,28)f32 #230=(1,116,28,28)f32
Conv2d                   stage3.0.branch1.0       1 1 230 728 bias=0 dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(2,2) @weight=(116,1,3,3)f32 #230=(1,116,28,28)f32 #728=(1,116,14,14)f32
BatchNorm2d              stage3.0.branch1.1       1 1 728 729 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #728=(1,116,14,14)f32 #729=(1,116,14,14)f32
Conv2d                   stage3.0.branch1.2       1 1 729 730 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #729=(1,116,14,14)f32 #730=(1,116,14,14)f32
BatchNorm2d              stage3.0.branch1.3       1 1 730 731 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #730=(1,116,14,14)f32 #731=(1,116,14,14)f32
ReLU                     stage3.0.branch1.4       1 1 731 732 #731=(1,116,14,14)f32 #732=(1,116,14,14)f32
Conv2d                   stage3.0.branch2.0       1 1 230 741 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #230=(1,116,28,28)f32 #741=(1,116,28,28)f32
BatchNorm2d              stage3.0.branch2.1       1 1 741 742 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #741=(1,116,28,28)f32 #742=(1,116,28,28)f32
ReLU                     stage3.0.branch2.2       1 1 742 743 #742=(1,116,28,28)f32 #743=(1,116,28,28)f32
Conv2d                   stage3.0.branch2.3       1 1 743 744 bias=0 dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(2,2) @weight=(116,1,3,3)f32 #743=(1,116,28,28)f32 #744=(1,116,14,14)f32
BatchNorm2d              stage3.0.branch2.4       1 1 744 745 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #744=(1,116,14,14)f32 #745=(1,116,14,14)f32
Conv2d                   stage3.0.branch2.5       1 1 745 746 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #745=(1,116,14,14)f32 #746=(1,116,14,14)f32
BatchNorm2d              stage3.0.branch2.6       1 1 746 747 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #746=(1,116,14,14)f32 #747=(1,116,14,14)f32
ReLU                     stage3.0.branch2.7       1 1 747 748 #747=(1,116,14,14)f32 #748=(1,116,14,14)f32
torch.cat                pnnx_146                 2 1 732 748 x.13 dim=1 #732=(1,116,14,14)f32 #748=(1,116,14,14)f32 #x.13=(1,232,14,14)f32
aten::view               pnnx_166                 1 1 x.13 x0.10 shape=[1,2,116,14,14] #x.13=(1,232,14,14)f32 #x0.10=(1,2,116,14,14)f32
torch.transpose          pnnx_169                 1 1 x0.10 x1.10 dim0=1 dim1=2 #x0.10=(1,2,116,14,14)f32 #x1.10=(1,116,2,14,14)f32
aten::view               pnnx_171                 1 1 x1.10 264 shape=[1,-1,14,14] #x1.10=(1,116,2,14,14)f32 #264=(1,232,14,14)f32
torch.chunk              pnnx_175                 1 2 264 x1.12 input.11 chunks=2 dim=1 #264=(1,232,14,14)f32 #x1.12=(1,116,14,14)f32 #input.11=(1,116,14,14)f32
Conv2d                   stage3.1.branch2.0       1 1 input.11 757 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #input.11=(1,116,14,14)f32 #757=(1,116,14,14)f32
BatchNorm2d              stage3.1.branch2.1       1 1 757 758 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #757=(1,116,14,14)f32 #758=(1,116,14,14)f32
ReLU                     stage3.1.branch2.2       1 1 758 759 #758=(1,116,14,14)f32 #759=(1,116,14,14)f32
Conv2d                   stage3.1.branch2.3       1 1 759 760 bias=0 dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(1,1) @weight=(116,1,3,3)f32 #759=(1,116,14,14)f32 #760=(1,116,14,14)f32
BatchNorm2d              stage3.1.branch2.4       1 1 760 761 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #760=(1,116,14,14)f32 #761=(1,116,14,14)f32
Conv2d                   stage3.1.branch2.5       1 1 761 762 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #761=(1,116,14,14)f32 #762=(1,116,14,14)f32
BatchNorm2d              stage3.1.branch2.6       1 1 762 763 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #762=(1,116,14,14)f32 #763=(1,116,14,14)f32
ReLU                     stage3.1.branch2.7       1 1 763 764 #763=(1,116,14,14)f32 #764=(1,116,14,14)f32
torch.cat                pnnx_180                 2 1 x1.12 764 x.15 dim=1 #x1.12=(1,116,14,14)f32 #764=(1,116,14,14)f32 #x.15=(1,232,14,14)f32
aten::view               pnnx_201                 1 1 x.15 x0.12 shape=[1,2,116,14,14] #x.15=(1,232,14,14)f32 #x0.12=(1,2,116,14,14)f32
torch.transpose          pnnx_204                 1 1 x0.12 x2.8 dim0=1 dim1=2 #x0.12=(1,2,116,14,14)f32 #x2.8=(1,116,2,14,14)f32
aten::view               pnnx_206                 1 1 x2.8 299 shape=[1,-1,14,14] #x2.8=(1,116,2,14,14)f32 #299=(1,232,14,14)f32
torch.chunk              pnnx_210                 1 2 299 x1.14 input.13 chunks=2 dim=1 #299=(1,232,14,14)f32 #x1.14=(1,116,14,14)f32 #input.13=(1,116,14,14)f32
Conv2d                   stage3.2.branch2.0       1 1 input.13 773 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #input.13=(1,116,14,14)f32 #773=(1,116,14,14)f32
BatchNorm2d              stage3.2.branch2.1       1 1 773 774 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #773=(1,116,14,14)f32 #774=(1,116,14,14)f32
ReLU                     stage3.2.branch2.2       1 1 774 775 #774=(1,116,14,14)f32 #775=(1,116,14,14)f32
Conv2d                   stage3.2.branch2.3       1 1 775 776 bias=0 dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(1,1) @weight=(116,1,3,3)f32 #775=(1,116,14,14)f32 #776=(1,116,14,14)f32
BatchNorm2d              stage3.2.branch2.4       1 1 776 777 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #776=(1,116,14,14)f32 #777=(1,116,14,14)f32
Conv2d                   stage3.2.branch2.5       1 1 777 778 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #777=(1,116,14,14)f32 #778=(1,116,14,14)f32
BatchNorm2d              stage3.2.branch2.6       1 1 778 779 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #778=(1,116,14,14)f32 #779=(1,116,14,14)f32
ReLU                     stage3.2.branch2.7       1 1 779 780 #779=(1,116,14,14)f32 #780=(1,116,14,14)f32
torch.cat                pnnx_215                 2 1 x1.14 780 x.17 dim=1 #x1.14=(1,116,14,14)f32 #780=(1,116,14,14)f32 #x.17=(1,232,14,14)f32
aten::view               pnnx_236                 1 1 x.17 x0.14 shape=[1,2,116,14,14] #x.17=(1,232,14,14)f32 #x0.14=(1,2,116,14,14)f32
torch.transpose          pnnx_239                 1 1 x0.14 x2.10 dim0=1 dim1=2 #x0.14=(1,2,116,14,14)f32 #x2.10=(1,116,2,14,14)f32
aten::view               pnnx_241                 1 1 x2.10 334 shape=[1,-1,14,14] #x2.10=(1,116,2,14,14)f32 #334=(1,232,14,14)f32
torch.chunk              pnnx_245                 1 2 334 x1.16 input.15 chunks=2 dim=1 #334=(1,232,14,14)f32 #x1.16=(1,116,14,14)f32 #input.15=(1,116,14,14)f32
Conv2d                   stage3.3.branch2.0       1 1 input.15 789 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #input.15=(1,116,14,14)f32 #789=(1,116,14,14)f32
BatchNorm2d              stage3.3.branch2.1       1 1 789 790 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #789=(1,116,14,14)f32 #790=(1,116,14,14)f32
ReLU                     stage3.3.branch2.2       1 1 790 791 #790=(1,116,14,14)f32 #791=(1,116,14,14)f32
Conv2d                   stage3.3.branch2.3       1 1 791 792 bias=0 dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(1,1) @weight=(116,1,3,3)f32 #791=(1,116,14,14)f32 #792=(1,116,14,14)f32
BatchNorm2d              stage3.3.branch2.4       1 1 792 793 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #792=(1,116,14,14)f32 #793=(1,116,14,14)f32
Conv2d                   stage3.3.branch2.5       1 1 793 794 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #793=(1,116,14,14)f32 #794=(1,116,14,14)f32
BatchNorm2d              stage3.3.branch2.6       1 1 794 795 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #794=(1,116,14,14)f32 #795=(1,116,14,14)f32
ReLU                     stage3.3.branch2.7       1 1 795 796 #795=(1,116,14,14)f32 #796=(1,116,14,14)f32
torch.cat                pnnx_250                 2 1 x1.16 796 x.19 dim=1 #x1.16=(1,116,14,14)f32 #796=(1,116,14,14)f32 #x.19=(1,232,14,14)f32
aten::view               pnnx_271                 1 1 x.19 x0.16 shape=[1,2,116,14,14] #x.19=(1,232,14,14)f32 #x0.16=(1,2,116,14,14)f32
torch.transpose          pnnx_274                 1 1 x0.16 x2.12 dim0=1 dim1=2 #x0.16=(1,2,116,14,14)f32 #x2.12=(1,116,2,14,14)f32
aten::view               pnnx_276                 1 1 x2.12 369 shape=[1,-1,14,14] #x2.12=(1,116,2,14,14)f32 #369=(1,232,14,14)f32
torch.chunk              pnnx_280                 1 2 369 x1.18 input.17 chunks=2 dim=1 #369=(1,232,14,14)f32 #x1.18=(1,116,14,14)f32 #input.17=(1,116,14,14)f32
Conv2d                   stage3.4.branch2.0       1 1 input.17 805 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #input.17=(1,116,14,14)f32 #805=(1,116,14,14)f32
BatchNorm2d              stage3.4.branch2.1       1 1 805 806 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #805=(1,116,14,14)f32 #806=(1,116,14,14)f32
ReLU                     stage3.4.branch2.2       1 1 806 807 #806=(1,116,14,14)f32 #807=(1,116,14,14)f32
Conv2d                   stage3.4.branch2.3       1 1 807 808 bias=0 dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(1,1) @weight=(116,1,3,3)f32 #807=(1,116,14,14)f32 #808=(1,116,14,14)f32
BatchNorm2d              stage3.4.branch2.4       1 1 808 809 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #808=(1,116,14,14)f32 #809=(1,116,14,14)f32
Conv2d                   stage3.4.branch2.5       1 1 809 810 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #809=(1,116,14,14)f32 #810=(1,116,14,14)f32
BatchNorm2d              stage3.4.branch2.6       1 1 810 811 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #810=(1,116,14,14)f32 #811=(1,116,14,14)f32
ReLU                     stage3.4.branch2.7       1 1 811 812 #811=(1,116,14,14)f32 #812=(1,116,14,14)f32
torch.cat                pnnx_285                 2 1 x1.18 812 x.21 dim=1 #x1.18=(1,116,14,14)f32 #812=(1,116,14,14)f32 #x.21=(1,232,14,14)f32
aten::view               pnnx_306                 1 1 x.21 x0.18 shape=[1,2,116,14,14] #x.21=(1,232,14,14)f32 #x0.18=(1,2,116,14,14)f32
torch.transpose          pnnx_309                 1 1 x0.18 x2.14 dim0=1 dim1=2 #x0.18=(1,2,116,14,14)f32 #x2.14=(1,116,2,14,14)f32
aten::view               pnnx_311                 1 1 x2.14 404 shape=[1,-1,14,14] #x2.14=(1,116,2,14,14)f32 #404=(1,232,14,14)f32
torch.chunk              pnnx_315                 1 2 404 x1.20 input.19 chunks=2 dim=1 #404=(1,232,14,14)f32 #x1.20=(1,116,14,14)f32 #input.19=(1,116,14,14)f32
Conv2d                   stage3.5.branch2.0       1 1 input.19 821 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #input.19=(1,116,14,14)f32 #821=(1,116,14,14)f32
BatchNorm2d              stage3.5.branch2.1       1 1 821 822 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #821=(1,116,14,14)f32 #822=(1,116,14,14)f32
ReLU                     stage3.5.branch2.2       1 1 822 823 #822=(1,116,14,14)f32 #823=(1,116,14,14)f32
Conv2d                   stage3.5.branch2.3       1 1 823 824 bias=0 dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(1,1) @weight=(116,1,3,3)f32 #823=(1,116,14,14)f32 #824=(1,116,14,14)f32
BatchNorm2d              stage3.5.branch2.4       1 1 824 825 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #824=(1,116,14,14)f32 #825=(1,116,14,14)f32
Conv2d                   stage3.5.branch2.5       1 1 825 826 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #825=(1,116,14,14)f32 #826=(1,116,14,14)f32
BatchNorm2d              stage3.5.branch2.6       1 1 826 827 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #826=(1,116,14,14)f32 #827=(1,116,14,14)f32
ReLU                     stage3.5.branch2.7       1 1 827 828 #827=(1,116,14,14)f32 #828=(1,116,14,14)f32
torch.cat                pnnx_320                 2 1 x1.20 828 x.23 dim=1 #x1.20=(1,116,14,14)f32 #828=(1,116,14,14)f32 #x.23=(1,232,14,14)f32
aten::view               pnnx_341                 1 1 x.23 x0.20 shape=[1,2,116,14,14] #x.23=(1,232,14,14)f32 #x0.20=(1,2,116,14,14)f32
torch.transpose          pnnx_344                 1 1 x0.20 x2.16 dim0=1 dim1=2 #x0.20=(1,2,116,14,14)f32 #x2.16=(1,116,2,14,14)f32
aten::view               pnnx_346                 1 1 x2.16 439 shape=[1,-1,14,14] #x2.16=(1,116,2,14,14)f32 #439=(1,232,14,14)f32
torch.chunk              pnnx_350                 1 2 439 x1.22 input.21 chunks=2 dim=1 #439=(1,232,14,14)f32 #x1.22=(1,116,14,14)f32 #input.21=(1,116,14,14)f32
Conv2d                   stage3.6.branch2.0       1 1 input.21 837 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #input.21=(1,116,14,14)f32 #837=(1,116,14,14)f32
BatchNorm2d              stage3.6.branch2.1       1 1 837 838 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #837=(1,116,14,14)f32 #838=(1,116,14,14)f32
ReLU                     stage3.6.branch2.2       1 1 838 839 #838=(1,116,14,14)f32 #839=(1,116,14,14)f32
Conv2d                   stage3.6.branch2.3       1 1 839 840 bias=0 dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(1,1) @weight=(116,1,3,3)f32 #839=(1,116,14,14)f32 #840=(1,116,14,14)f32
BatchNorm2d              stage3.6.branch2.4       1 1 840 841 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #840=(1,116,14,14)f32 #841=(1,116,14,14)f32
Conv2d                   stage3.6.branch2.5       1 1 841 842 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #841=(1,116,14,14)f32 #842=(1,116,14,14)f32
BatchNorm2d              stage3.6.branch2.6       1 1 842 843 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #842=(1,116,14,14)f32 #843=(1,116,14,14)f32
ReLU                     stage3.6.branch2.7       1 1 843 844 #843=(1,116,14,14)f32 #844=(1,116,14,14)f32
torch.cat                pnnx_355                 2 1 x1.22 844 x.25 dim=1 #x1.22=(1,116,14,14)f32 #844=(1,116,14,14)f32 #x.25=(1,232,14,14)f32
aten::view               pnnx_376                 1 1 x.25 x0.22 shape=[1,2,116,14,14] #x.25=(1,232,14,14)f32 #x0.22=(1,2,116,14,14)f32
torch.transpose          pnnx_379                 1 1 x0.22 x2.18 dim0=1 dim1=2 #x0.22=(1,2,116,14,14)f32 #x2.18=(1,116,2,14,14)f32
aten::view               pnnx_381                 1 1 x2.18 474 shape=[1,-1,14,14] #x2.18=(1,116,2,14,14)f32 #474=(1,232,14,14)f32
torch.chunk              pnnx_385                 1 2 474 x1.24 input.23 chunks=2 dim=1 #474=(1,232,14,14)f32 #x1.24=(1,116,14,14)f32 #input.23=(1,116,14,14)f32
Conv2d                   stage3.7.branch2.0       1 1 input.23 853 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #input.23=(1,116,14,14)f32 #853=(1,116,14,14)f32
BatchNorm2d              stage3.7.branch2.1       1 1 853 854 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #853=(1,116,14,14)f32 #854=(1,116,14,14)f32
ReLU                     stage3.7.branch2.2       1 1 854 855 #854=(1,116,14,14)f32 #855=(1,116,14,14)f32
Conv2d                   stage3.7.branch2.3       1 1 855 856 bias=0 dilation=(1,1) groups=116 in_channels=116 kernel_size=(3,3) out_channels=116 padding=(1,1) stride=(1,1) @weight=(116,1,3,3)f32 #855=(1,116,14,14)f32 #856=(1,116,14,14)f32
BatchNorm2d              stage3.7.branch2.4       1 1 856 857 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #856=(1,116,14,14)f32 #857=(1,116,14,14)f32
Conv2d                   stage3.7.branch2.5       1 1 857 858 bias=0 dilation=(1,1) groups=1 in_channels=116 kernel_size=(1,1) out_channels=116 padding=(0,0) stride=(1,1) @weight=(116,116,1,1)f32 #857=(1,116,14,14)f32 #858=(1,116,14,14)f32
BatchNorm2d              stage3.7.branch2.6       1 1 858 859 affine=1 eps=1.000000e-05 num_features=116 @bias=(116)f32 @running_mean=(116)f32 @running_var=(116)f32 @weight=(116)f32 #858=(1,116,14,14)f32 #859=(1,116,14,14)f32
ReLU                     stage3.7.branch2.7       1 1 859 860 #859=(1,116,14,14)f32 #860=(1,116,14,14)f32
torch.cat                pnnx_390                 2 1 x1.24 860 x.27 dim=1 #x1.24=(1,116,14,14)f32 #860=(1,116,14,14)f32 #x.27=(1,232,14,14)f32
aten::view               pnnx_411                 1 1 x.27 x0.24 shape=[1,2,116,14,14] #x.27=(1,232,14,14)f32 #x0.24=(1,2,116,14,14)f32
torch.transpose          pnnx_414                 1 1 x0.24 x2.20 dim0=1 dim1=2 #x0.24=(1,2,116,14,14)f32 #x2.20=(1,116,2,14,14)f32
aten::view               pnnx_416                 1 1 x2.20 509 shape=[1,-1,14,14] #x2.20=(1,116,2,14,14)f32 #509=(1,232,14,14)f32
Conv2d                   stage4.0.branch1.0       1 1 509 866 bias=0 dilation=(1,1) groups=232 in_channels=232 kernel_size=(3,3) out_channels=232 padding=(1,1) stride=(2,2) @weight=(232,1,3,3)f32 #509=(1,232,14,14)f32 #866=(1,232,7,7)f32
BatchNorm2d              stage4.0.branch1.1       1 1 866 867 affine=1 eps=1.000000e-05 num_features=232 @bias=(232)f32 @running_mean=(232)f32 @running_var=(232)f32 @weight=(232)f32 #866=(1,232,7,7)f32 #867=(1,232,7,7)f32
Conv2d                   stage4.0.branch1.2       1 1 867 868 bias=0 dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @weight=(232,232,1,1)f32 #867=(1,232,7,7)f32 #868=(1,232,7,7)f32
BatchNorm2d              stage4.0.branch1.3       1 1 868 869 affine=1 eps=1.000000e-05 num_features=232 @bias=(232)f32 @running_mean=(232)f32 @running_var=(232)f32 @weight=(232)f32 #868=(1,232,7,7)f32 #869=(1,232,7,7)f32
ReLU                     stage4.0.branch1.4       1 1 869 870 #869=(1,232,7,7)f32 #870=(1,232,7,7)f32
Conv2d                   stage4.0.branch2.0       1 1 509 879 bias=0 dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @weight=(232,232,1,1)f32 #509=(1,232,14,14)f32 #879=(1,232,14,14)f32
BatchNorm2d              stage4.0.branch2.1       1 1 879 880 affine=1 eps=1.000000e-05 num_features=232 @bias=(232)f32 @running_mean=(232)f32 @running_var=(232)f32 @weight=(232)f32 #879=(1,232,14,14)f32 #880=(1,232,14,14)f32
ReLU                     stage4.0.branch2.2       1 1 880 881 #880=(1,232,14,14)f32 #881=(1,232,14,14)f32
Conv2d                   stage4.0.branch2.3       1 1 881 882 bias=0 dilation=(1,1) groups=232 in_channels=232 kernel_size=(3,3) out_channels=232 padding=(1,1) stride=(2,2) @weight=(232,1,3,3)f32 #881=(1,232,14,14)f32 #882=(1,232,7,7)f32
BatchNorm2d              stage4.0.branch2.4       1 1 882 883 affine=1 eps=1.000000e-05 num_features=232 @bias=(232)f32 @running_mean=(232)f32 @running_var=(232)f32 @weight=(232)f32 #882=(1,232,7,7)f32 #883=(1,232,7,7)f32
Conv2d                   stage4.0.branch2.5       1 1 883 884 bias=0 dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @weight=(232,232,1,1)f32 #883=(1,232,7,7)f32 #884=(1,232,7,7)f32
BatchNorm2d              stage4.0.branch2.6       1 1 884 885 affine=1 eps=1.000000e-05 num_features=232 @bias=(232)f32 @running_mean=(232)f32 @running_var=(232)f32 @weight=(232)f32 #884=(1,232,7,7)f32 #885=(1,232,7,7)f32
ReLU                     stage4.0.branch2.7       1 1 885 886 #885=(1,232,7,7)f32 #886=(1,232,7,7)f32
torch.cat                pnnx_423                 2 1 870 886 x.29 dim=1 #870=(1,232,7,7)f32 #886=(1,232,7,7)f32 #x.29=(1,464,7,7)f32
aten::view               pnnx_443                 1 1 x.29 x0.26 shape=[1,2,232,7,7] #x.29=(1,464,7,7)f32 #x0.26=(1,2,232,7,7)f32
torch.transpose          pnnx_446                 1 1 x0.26 x1.26 dim0=1 dim1=2 #x0.26=(1,2,232,7,7)f32 #x1.26=(1,232,2,7,7)f32
aten::view               pnnx_448                 1 1 x1.26 543 shape=[1,-1,7,7] #x1.26=(1,232,2,7,7)f32 #543=(1,464,7,7)f32
torch.chunk              pnnx_452                 1 2 543 x1.28 input.25 chunks=2 dim=1 #543=(1,464,7,7)f32 #x1.28=(1,232,7,7)f32 #input.25=(1,232,7,7)f32
Conv2d                   stage4.1.branch2.0       1 1 input.25 895 bias=0 dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @weight=(232,232,1,1)f32 #input.25=(1,232,7,7)f32 #895=(1,232,7,7)f32
BatchNorm2d              stage4.1.branch2.1       1 1 895 896 affine=1 eps=1.000000e-05 num_features=232 @bias=(232)f32 @running_mean=(232)f32 @running_var=(232)f32 @weight=(232)f32 #895=(1,232,7,7)f32 #896=(1,232,7,7)f32
ReLU                     stage4.1.branch2.2       1 1 896 897 #896=(1,232,7,7)f32 #897=(1,232,7,7)f32
Conv2d                   stage4.1.branch2.3       1 1 897 898 bias=0 dilation=(1,1) groups=232 in_channels=232 kernel_size=(3,3) out_channels=232 padding=(1,1) stride=(1,1) @weight=(232,1,3,3)f32 #897=(1,232,7,7)f32 #898=(1,232,7,7)f32
BatchNorm2d              stage4.1.branch2.4       1 1 898 899 affine=1 eps=1.000000e-05 num_features=232 @bias=(232)f32 @running_mean=(232)f32 @running_var=(232)f32 @weight=(232)f32 #898=(1,232,7,7)f32 #899=(1,232,7,7)f32
Conv2d                   stage4.1.branch2.5       1 1 899 900 bias=0 dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @weight=(232,232,1,1)f32 #899=(1,232,7,7)f32 #900=(1,232,7,7)f32
BatchNorm2d              stage4.1.branch2.6       1 1 900 901 affine=1 eps=1.000000e-05 num_features=232 @bias=(232)f32 @running_mean=(232)f32 @running_var=(232)f32 @weight=(232)f32 #900=(1,232,7,7)f32 #901=(1,232,7,7)f32
ReLU                     stage4.1.branch2.7       1 1 901 902 #901=(1,232,7,7)f32 #902=(1,232,7,7)f32
torch.cat                pnnx_457                 2 1 x1.28 902 x.31 dim=1 #x1.28=(1,232,7,7)f32 #902=(1,232,7,7)f32 #x.31=(1,464,7,7)f32
aten::view               pnnx_478                 1 1 x.31 x0.28 shape=[1,2,232,7,7] #x.31=(1,464,7,7)f32 #x0.28=(1,2,232,7,7)f32
torch.transpose          pnnx_481                 1 1 x0.28 x2.22 dim0=1 dim1=2 #x0.28=(1,2,232,7,7)f32 #x2.22=(1,232,2,7,7)f32
aten::view               pnnx_483                 1 1 x2.22 578 shape=[1,-1,7,7] #x2.22=(1,232,2,7,7)f32 #578=(1,464,7,7)f32
torch.chunk              pnnx_487                 1 2 578 x1.30 input.27 chunks=2 dim=1 #578=(1,464,7,7)f32 #x1.30=(1,232,7,7)f32 #input.27=(1,232,7,7)f32
Conv2d                   stage4.2.branch2.0       1 1 input.27 911 bias=0 dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @weight=(232,232,1,1)f32 #input.27=(1,232,7,7)f32 #911=(1,232,7,7)f32
BatchNorm2d              stage4.2.branch2.1       1 1 911 912 affine=1 eps=1.000000e-05 num_features=232 @bias=(232)f32 @running_mean=(232)f32 @running_var=(232)f32 @weight=(232)f32 #911=(1,232,7,7)f32 #912=(1,232,7,7)f32
ReLU                     stage4.2.branch2.2       1 1 912 913 #912=(1,232,7,7)f32 #913=(1,232,7,7)f32
Conv2d                   stage4.2.branch2.3       1 1 913 914 bias=0 dilation=(1,1) groups=232 in_channels=232 kernel_size=(3,3) out_channels=232 padding=(1,1) stride=(1,1) @weight=(232,1,3,3)f32 #913=(1,232,7,7)f32 #914=(1,232,7,7)f32
BatchNorm2d              stage4.2.branch2.4       1 1 914 915 affine=1 eps=1.000000e-05 num_features=232 @bias=(232)f32 @running_mean=(232)f32 @running_var=(232)f32 @weight=(232)f32 #914=(1,232,7,7)f32 #915=(1,232,7,7)f32
Conv2d                   stage4.2.branch2.5       1 1 915 916 bias=0 dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @weight=(232,232,1,1)f32 #915=(1,232,7,7)f32 #916=(1,232,7,7)f32
BatchNorm2d              stage4.2.branch2.6       1 1 916 917 affine=1 eps=1.000000e-05 num_features=232 @bias=(232)f32 @running_mean=(232)f32 @running_var=(232)f32 @weight=(232)f32 #916=(1,232,7,7)f32 #917=(1,232,7,7)f32
ReLU                     stage4.2.branch2.7       1 1 917 918 #917=(1,232,7,7)f32 #918=(1,232,7,7)f32
torch.cat                pnnx_492                 2 1 x1.30 918 x.33 dim=1 #x1.30=(1,232,7,7)f32 #918=(1,232,7,7)f32 #x.33=(1,464,7,7)f32
aten::view               pnnx_513                 1 1 x.33 x0.30 shape=[1,2,232,7,7] #x.33=(1,464,7,7)f32 #x0.30=(1,2,232,7,7)f32
torch.transpose          pnnx_516                 1 1 x0.30 x2.24 dim0=1 dim1=2 #x0.30=(1,2,232,7,7)f32 #x2.24=(1,232,2,7,7)f32
aten::view               pnnx_518                 1 1 x2.24 613 shape=[1,-1,7,7] #x2.24=(1,232,2,7,7)f32 #613=(1,464,7,7)f32
torch.chunk              pnnx_522                 1 2 613 x1.1 input.1 chunks=2 dim=1 #613=(1,464,7,7)f32 #x1.1=(1,232,7,7)f32 #input.1=(1,232,7,7)f32
Conv2d                   stage4.3.branch2.0       1 1 input.1 927 bias=0 dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @weight=(232,232,1,1)f32 #input.1=(1,232,7,7)f32 #927=(1,232,7,7)f32
BatchNorm2d              stage4.3.branch2.1       1 1 927 928 affine=1 eps=1.000000e-05 num_features=232 @bias=(232)f32 @running_mean=(232)f32 @running_var=(232)f32 @weight=(232)f32 #927=(1,232,7,7)f32 #928=(1,232,7,7)f32
ReLU                     stage4.3.branch2.2       1 1 928 929 #928=(1,232,7,7)f32 #929=(1,232,7,7)f32
Conv2d                   stage4.3.branch2.3       1 1 929 930 bias=0 dilation=(1,1) groups=232 in_channels=232 kernel_size=(3,3) out_channels=232 padding=(1,1) stride=(1,1) @weight=(232,1,3,3)f32 #929=(1,232,7,7)f32 #930=(1,232,7,7)f32
BatchNorm2d              stage4.3.branch2.4       1 1 930 931 affine=1 eps=1.000000e-05 num_features=232 @bias=(232)f32 @running_mean=(232)f32 @running_var=(232)f32 @weight=(232)f32 #930=(1,232,7,7)f32 #931=(1,232,7,7)f32
Conv2d                   stage4.3.branch2.5       1 1 931 932 bias=0 dilation=(1,1) groups=1 in_channels=232 kernel_size=(1,1) out_channels=232 padding=(0,0) stride=(1,1) @weight=(232,232,1,1)f32 #931=(1,232,7,7)f32 #932=(1,232,7,7)f32
BatchNorm2d              stage4.3.branch2.6       1 1 932 933 affine=1 eps=1.000000e-05 num_features=232 @bias=(232)f32 @running_mean=(232)f32 @running_var=(232)f32 @weight=(232)f32 #932=(1,232,7,7)f32 #933=(1,232,7,7)f32
ReLU                     stage4.3.branch2.7       1 1 933 934 #933=(1,232,7,7)f32 #934=(1,232,7,7)f32
torch.cat                pnnx_527                 2 1 x1.1 934 x.1 dim=1 #x1.1=(1,232,7,7)f32 #934=(1,232,7,7)f32 #x.1=(1,464,7,7)f32
aten::view               pnnx_548                 1 1 x.1 x0.1 shape=[1,2,232,7,7] #x.1=(1,464,7,7)f32 #x0.1=(1,2,232,7,7)f32
torch.transpose          pnnx_551                 1 1 x0.1 x2.1 dim0=1 dim1=2 #x0.1=(1,2,232,7,7)f32 #x2.1=(1,232,2,7,7)f32
aten::view               pnnx_553                 1 1 x2.1 648 shape=[1,-1,7,7] #x2.1=(1,232,2,7,7)f32 #648=(1,464,7,7)f32
Conv2d                   conv5.0                  1 1 648 89 bias=0 dilation=(1,1) groups=1 in_channels=464 kernel_size=(1,1) out_channels=1024 padding=(0,0) stride=(1,1) @weight=(1024,464,1,1)f32 #648=(1,464,7,7)f32 #89=(1,1024,7,7)f32
BatchNorm2d              conv5.1                  1 1 89 90 affine=1 eps=1.000000e-05 num_features=1024 @bias=(1024)f32 @running_mean=(1024)f32 @running_var=(1024)f32 @weight=(1024)f32 #89=(1,1024,7,7)f32 #90=(1,1024,7,7)f32
ReLU                     conv5.2                  1 1 90 91 #90=(1,1024,7,7)f32 #91=(1,1024,7,7)f32
torch.mean               pnnx_555                 1 1 91 input.3 dim=[2,3] keepdim=0 #91=(1,1024,7,7)f32 #input.3=(1,1024)f32
Linear                   fc                       1 1 input.3 38 bias=1 in_features=1024 out_features=1000 @bias=(1000)f32 @weight=(1000,1024)f32 #input.3=(1,1024)f32 #38=(1,1000)f32
Output                   pnnx_output_0            1 0 38 #38=(1,1000)f32
